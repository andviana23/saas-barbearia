#!/bin/bash
# ===============================================
# BACKUP L√ìGICO DI√ÅRIO - SUPABASE
# Data: 25/08/2025
# Descri√ß√£o: Backup automatizado com S3 upload
# ===============================================

set -e

# Configura√ß√µes
DB_URL="${SUPABASE_DB_URL}"
BACKUP_DIR="/tmp/backups/$(date +%Y-%m-%d)"
S3_BUCKET="${BACKUP_S3_BUCKET:-saas-barbearia-backups}"
RETENTION_DAYS="${BACKUP_RETENTION_DAYS:-30}"

# Fun√ß√µes auxiliares
log() {
  echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

error_exit() {
  log "‚ùå ERRO: $1"
  exit 1
}

# Verificar depend√™ncias
command -v pg_dump >/dev/null 2>&1 || error_exit "pg_dump n√£o encontrado"
command -v aws >/dev/null 2>&1 || error_exit "AWS CLI n√£o encontrado"

# Verificar vari√°veis obrigat√≥rias
[ -z "${DB_URL}" ] && error_exit "SUPABASE_DB_URL n√£o configurado"

log "üöÄ Iniciando backup di√°rio"
log "üìÅ Diret√≥rio: ${BACKUP_DIR}"
log "‚òÅÔ∏è  S3 Bucket: ${S3_BUCKET}"

# Criar diret√≥rio de backup
mkdir -p "${BACKUP_DIR}"

# 1. Backup das tabelas cr√≠ticas (dados apenas)
log "üìã 1/5 Backup de tabelas cr√≠ticas..."
pg_dump "${DB_URL}" \
  --schema=public \
  --data-only \
  --no-owner \
  --no-privileges \
  --table=unidades \
  --table=profiles \
  --table=profissionais \
  --table=clientes \
  --table=servicos \
  --table=agendamentos \
  --table=appointments \
  --table=pagamentos \
  --table=subscriptions \
  --table=assinaturas \
  > "${BACKUP_DIR}/critical-data.sql" || error_exit "Falha no backup de dados cr√≠ticos"

log "‚úÖ Tabelas cr√≠ticas: $(wc -l < "${BACKUP_DIR}/critical-data.sql") linhas"

# 2. Backup completo (estrutura + dados)
log "üìã 2/5 Backup completo (estrutura + dados)..."
pg_dump "${DB_URL}" \
  --schema=public \
  --create \
  --clean \
  --if-exists \
  --no-owner \
  --no-privileges \
  > "${BACKUP_DIR}/full-backup.sql" || error_exit "Falha no backup completo"

log "‚úÖ Backup completo: $(wc -l < "${BACKUP_DIR}/full-backup.sql") linhas"

# 3. Backup de configura√ß√µes e auth
log "üìã 3/5 Backup de configura√ß√µes..."
pg_dump "${DB_URL}" \
  --schema=auth \
  --data-only \
  --no-owner \
  --no-privileges \
  > "${BACKUP_DIR}/auth-backup.sql" 2>/dev/null || log "‚ö†Ô∏è  Schema auth n√£o encontrado (normal em dev)"

# Backup de fun√ß√µes e pol√≠ticas RLS
pg_dump "${DB_URL}" \
  --schema=public \
  --schema-only \
  --no-owner \
  --no-privileges \
  > "${BACKUP_DIR}/schema-backup.sql" || error_exit "Falha no backup de schema"

log "‚úÖ Configura√ß√µes backup conclu√≠do"

# 4. Verifica√ß√£o de integridade
log "üìã 4/5 Verificando integridade dos backups..."

# Verificar se arquivos n√£o est√£o vazios
for file in critical-data.sql full-backup.sql schema-backup.sql; do
  if [ ! -s "${BACKUP_DIR}/${file}" ]; then
    error_exit "Arquivo ${file} est√° vazio"
  fi
done

# Verificar estrutura SQL b√°sica
if ! grep -q "CREATE TABLE" "${BACKUP_DIR}/schema-backup.sql"; then
  error_exit "Schema backup n√£o cont√©m CREATE TABLE statements"
fi

if ! grep -q "INSERT INTO" "${BACKUP_DIR}/critical-data.sql"; then
  log "‚ö†Ô∏è  Backup de dados cr√≠ticos n√£o cont√©m INSERT statements (pode estar vazio)"
fi

log "‚úÖ Integridade verificada"

# 5. Compress√£o e upload
log "üìã 5/5 Compress√£o e upload para S3..."

# Adicionar metadata ao backup
cat > "${BACKUP_DIR}/backup-info.json" <<EOF
{
  "backup_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "database_url_hash": "$(echo -n "${DB_URL}" | sha256sum | cut -d' ' -f1)",
  "backup_type": "daily_logical",
  "files": [
    "critical-data.sql",
    "full-backup.sql", 
    "schema-backup.sql",
    "auth-backup.sql"
  ],
  "retention_days": ${RETENTION_DAYS},
  "version": "1.0"
}
EOF

# Comprimir backup
cd "$(dirname "${BACKUP_DIR}")"
BACKUP_NAME="$(basename "${BACKUP_DIR}").tar.gz"
tar -czf "${BACKUP_NAME}" "$(basename "${BACKUP_DIR}")" || error_exit "Falha na compress√£o"

# Upload para S3
aws s3 cp "${BACKUP_NAME}" "s3://${S3_BUCKET}/daily/" \
  --metadata "backup-date=$(date -u +%Y-%m-%dT%H:%M:%SZ),retention-days=${RETENTION_DAYS}" \
  || error_exit "Falha no upload para S3"

# Verificar upload
UPLOADED_SIZE=$(aws s3 ls "s3://${S3_BUCKET}/daily/${BACKUP_NAME}" --human-readable | awk '{print $3" "$4}')
log "‚úÖ Upload conclu√≠do: ${UPLOADED_SIZE}"

# Limpeza local
rm -rf "${BACKUP_DIR}" "${BACKUP_NAME}"

# Limpeza de backups antigos no S3
log "üßπ Removendo backups antigos (>${RETENTION_DAYS} dias)..."
CUTOFF_DATE=$(date -u -d "${RETENTION_DAYS} days ago" +%Y-%m-%d)

aws s3 ls "s3://${S3_BUCKET}/daily/" | while read -r line; do
  BACKUP_DATE=$(echo "$line" | awk '{print $1}')
  BACKUP_FILE=$(echo "$line" | awk '{print $4}')
  
  if [[ "${BACKUP_DATE}" < "${CUTOFF_DATE}" && -n "${BACKUP_FILE}" ]]; then
    log "üóëÔ∏è  Removendo backup antigo: ${BACKUP_FILE}"
    aws s3 rm "s3://${S3_BUCKET}/daily/${BACKUP_FILE}" || log "‚ö†Ô∏è  Falha ao remover ${BACKUP_FILE}"
  fi
done

# Estat√≠sticas finais
TOTAL_BACKUPS=$(aws s3 ls "s3://${S3_BUCKET}/daily/" | wc -l)
log "üìä Total de backups mantidos: ${TOTAL_BACKUPS}"

log "‚úÖ BACKUP DI√ÅRIO CONCLU√çDO COM SUCESSO"
log "üìÅ Localiza√ß√£o: s3://${S3_BUCKET}/daily/${BACKUP_NAME}"
log "‚è∞ Pr√≥ximo backup: $(date -d 'tomorrow 02:00' '+%Y-%m-%d %H:%M')"

# Notificar sucesso (opcional)
if [ -n "${BACKUP_SUCCESS_WEBHOOK}" ]; then
  curl -X POST "${BACKUP_SUCCESS_WEBHOOK}" \
    -H "Content-Type: application/json" \
    -d "{\"text\":\"‚úÖ Backup di√°rio conclu√≠do: ${BACKUP_NAME} (${UPLOADED_SIZE})\"}" \
    >/dev/null 2>&1 || log "‚ö†Ô∏è  Falha ao enviar notifica√ß√£o de sucesso"
fi

exit 0